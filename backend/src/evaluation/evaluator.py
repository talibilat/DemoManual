"""
Evaluation Module for RAG System Responses.

This module provides comprehensive evaluation functions to assess the quality and reliability
of responses generated by the Retrieval-Augmented Generation (RAG) system. The evaluation
considers multiple factors including semantic similarity to reference answers and LLM-based
assessments of factual accuracy, relevance, completeness, and context usage.

The evaluation results are scored on a scale from 0 to 1 (normalized) and saved to JSON files
for future analysis and comparison.
"""
from typing import Dict, Any, List
import os
import json
from datetime import datetime
import logging
from .metrics import calculate_similarity, evaluate_with_llm

# Configure logging
logger = logging.getLogger(__name__)

# Create results directory
RESULTS_DIR = os.path.join("results", "evaluations")
os.makedirs(RESULTS_DIR, exist_ok=True)

def _save_results(results: Dict[str, Any]):
    """
    Save evaluation results to a JSON file with timestamp.
    
    The results are stored in a standardized format in the evaluations directory,
    allowing for later analysis and comparison of system performance over time.
    
    Args:
        results (Dict[str, Any]): Evaluation results dictionary containing scores,
                                  metadata, and evaluation details
    """
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = os.path.join(RESULTS_DIR, f"evaluation_{timestamp}.json")
    
    try:
        with open(filename, 'w') as f:
            json.dump(results, f, indent=2)
        logger.info(f"Results saved to {filename}")
    except Exception as e:
        logger.error(f"Error saving results: {str(e)}")

async def evaluate_response(
    question: str,
    response: str,
    context: List[str],
    reference_answer: str = None
) -> Dict[str, Any]:
    """
    Perform comprehensive evaluation of a RAG response using multiple metrics.
    
    This function combines different evaluation methods to provide a holistic assessment
    of the RAG system's response quality:
    
    1. Semantic similarity: Measures how closely the response matches a reference answer
    2. LLM-based evaluation: Uses an LLM to assess factual accuracy, relevance, etc.
    3. Overall confidence: Combines all metrics into a single confidence score
    
    Args:
        question (str): The original user question
        response (str): The generated response from the RAG system
        context (List[str]): The retrieved context passages used to generate the response
        reference_answer (str, optional): A reference (ground truth) answer for comparison
        
    Returns:
        Dict[str, Any]: Complete evaluation results dictionary containing:
            - timestamp: When the evaluation was performed
            - question: The original question
            - response: The generated response
            - context: The retrieved context passages
            - semantic_similarity: Score comparing response to reference answer (if provided)
            - llm_evaluation: Results from LLM-based evaluation (scores and explanations)
            - overall_confidence: Combined confidence score (0-1 scale)
    """
    results = {
        "timestamp": datetime.now().isoformat(),
        "question": question,
        "response": response,
        "context": context
    }
    
    # Semantic similarity evaluation - compare to reference answer if available
    if reference_answer:
        results["semantic_similarity"] = calculate_similarity(
            response, reference_answer
        )
    
    # LLM-based evaluation - assess factual accuracy, relevance, completeness
    llm_results = await evaluate_with_llm(question, response, context)
    results["llm_evaluation"] = llm_results
    
    # Calculate overall confidence from combined metrics
    # We normalize all scores to 0-1 range and take their average
    if llm_results["scores"]:
        confidence_factors = [
            results.get("semantic_similarity", 0),
            *[score/10.0 for score in llm_results["scores"].values()]  # Normalize to 0-1 range
        ]
        results["overall_confidence"] = sum(
            score for score in confidence_factors if score is not None
        ) / len(confidence_factors)
    else:
        results["overall_confidence"] = 0.0
    
    # Save results to JSON file for future analysis
    _save_results(results)
    
    return results 